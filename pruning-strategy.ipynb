{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec25fe-bed3-43a9-94e8-22413302a7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from node_attribution.node_attribution import get_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "289d2a47-0d93-43ad-897b-3ae05dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"560m\"\n",
    "data = [\"Hello, I am an AlexPrize chatbot\"]\n",
    "prune_percent = 0.10\n",
    "num_params_to_prune = 816115712 * prune_percent\n",
    "\n",
    "head_dim = 64\n",
    "num_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baf45e79-30ba-4500-8cba-31e5b931a41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81611571.2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8efbe0a3-048c-4ca6-a77e-dfe9315161ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading bigscience/bloom-560m.\n",
      "                    Num Transformer Blocks: 24\n",
      "                    Num Attention Heads: 16\n",
      "                    Head Dim: 64\n",
      "                    Hidden Size: 1024\n",
      "                    Base Model Param Count: 559,214,592\n",
      "                    Total Param Count (w/ LM Head): 816,115,712\n"
     ]
    }
   ],
   "source": [
    "# Get attributions\n",
    "avg_contributions, max_contributions, model, model_params = get_attributions(model_size, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e880a64-b350-4423-9298-5ef32f86976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "params_to_index = {}\n",
    "for param_name in model_params.keys():\n",
    "    if \"bias\" not in param_name:\n",
    "        params_to_index[param_name] = index\n",
    "        index += 1\n",
    "    \n",
    "index_to_params = {params_to_index[param_name]: param_name for param_name in params_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "830cf301-0660-43f4-839c-02016f3bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shape_map = {}\n",
    "layer_names, contribution_tensors = zip(*avg_contributions.items())\n",
    "num_layers = len(layer_names)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer_name = layer_names[i]\n",
    "    layer_size = contribution_tensors[i].shape[0]\n",
    "    \n",
    "    if i != 0:\n",
    "        next_layer_size = contribution_tensors[i - 1].shape[0]\n",
    "    else:\n",
    "        next_layer_size = 250880\n",
    "        \n",
    "    \n",
    "    if i != (num_layers - 1):\n",
    "        prev_layer_size = contribution_tensors[i + 1].shape[0]\n",
    "    else:\n",
    "        prev_layer_size = 1024\n",
    "        \n",
    "    layer_shape_map[layer_name] = {\n",
    "        \"prev_layer_size\": prev_layer_size,\n",
    "        \"next_layer_size\": next_layer_size,\n",
    "        \"current_layer_size\": layer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52304b74-b166-4c13-a95f-2eafc0c3a705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    for node_id, node in enumerate(contribution_tensor.tolist()):\n",
    "        node_name = f\"{layer_name}.{node_id}\"\n",
    "        all_nodes.append((node_name, node))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee0d3a6-0b98-4e18-9940-34b0f3ed7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all the nodes\n",
    "all_nodes.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7061e785-c991-4e60-b4c3-80bfb778cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = []\n",
    "\n",
    "# Figure out which layers have the most to prune\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    \n",
    "    # Don't prune self_attention.dense.weight directly, use value matrix to decide what to prune\n",
    "    if \"self_attention.dense.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"mlp.dense_h_to_4h.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"self_attention.query_key_value.weight\" in layer_name:\n",
    "        continue\n",
    "    \n",
    "    # Get average contribution over the whole layer\n",
    "    mean_contribution = torch.mean(contribution_tensor, 0).item()\n",
    "    all_layers.append((layer_name, mean_contribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6568baf2-3382-4e41-a309-e20b827d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f75b8e-7803-464c-b368-21c453ba0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_masks = {}\n",
    "num_params_pruned = 0\n",
    "node_num = 0\n",
    "\n",
    "while num_params_pruned < num_params_to_prune:\n",
    "    lowest_contr_layer_name = all_layers[0][0]               \n",
    "    shapes = layer_shape_map[lowest_contr_layer_name]\n",
    "    \n",
    "    # If the layer is a query_key_value_fused_output layer\n",
    "    if \"query_key_value_fused_output\" in lowest_contr_layer_name:\n",
    "\n",
    "        # Build pruning mask\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "            layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "\n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            st = torch.sort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "        # Prune 3 * num head nodes at once, (this will decrease head dim by 1 for each atten head in this layer)\n",
    "        # Make sure each of the 3 nodes are in the 3 different query, key, and value regions\n",
    "\n",
    "        # TODO: Find a more efficient way of doing this\n",
    "        for head in range(num_heads):\n",
    "            for qkv in range(3):\n",
    "                index_to_mask = sorted_contributions[head][qkv][num_pruned]\n",
    "                mask[head][qkv][index_to_mask] = 1\n",
    "\n",
    "        layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "        num_params_pruned += (shapes[\"prev_layer_size\"] * num_heads * 3)\n",
    "        num_params_pruned += (shapes[\"next_layer_size\"] * num_heads * 3)\n",
    "        node_num += (num_heads * 3)\n",
    "\n",
    "    else:\n",
    "        # Prune one node at time\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "        index_to_mask = sorted_contributions[num_pruned]\n",
    "        mask[index_to_mask] = 1\n",
    "        \n",
    "        layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "        num_params_pruned += shapes[\"prev_layer_size\"]\n",
    "        num_params_pruned += shapes[\"next_layer_size\"]\n",
    "        node_num += 1\n",
    "\n",
    "    # Apply mask and update the layer mean in \"all_layers\"\n",
    "    mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=mask)\n",
    "    new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    # print(all_layers[0])\n",
    "    all_layers[0] = (lowest_contr_layer_name, new_layer_contr_score)\n",
    "    # print(all_layers[0])\n",
    "    # print(f\"Num params removed: {num_params_pruned}\")\n",
    "    # print(f\"Num Nodes removed: {node_num}\")\n",
    "    # print(\"=====\")\n",
    "    \n",
    "    # re-sort layers now that this one has been pruned and pick the lowest contributing layer again\n",
    "    all_layers.sort(key = lambda x:x[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc0d48da-8f45-4f78-a502-ed4e413b6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Line up weights to prune and weights in the state dict\n",
    "mask_index = 0\n",
    "sorted_weight_index = 1\n",
    "pruned_model_params = model_params.copy()\n",
    "\n",
    "for layer_name in layer_masks.keys():\n",
    "    if layer_name == \"transformer.h.0.self_attention.query_key_value.weight\":\n",
    "        continue\n",
    "    elif \"query_key_value_fused_output\" in layer_name:   \n",
    "        # Prune as input\n",
    "        # Look at value matrix to decide what should be droped in \"self_attention.dense.weight\"\n",
    "        value_reshape_mask = layer_masks[layer_name][mask_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        num_nodes_to_drop = int(sum(value_reshape_mask).item())\n",
    "        value_indices = layer_masks[layer_name][sorted_weight_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        value_keep_index = torch.sort(value_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        dense_layer_name = layer_name.replace(\"query_key_value_fused_output\", \"dense\")\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[dense_layer_name], -1, value_keep_index)\n",
    "        pruned_model_params[dense_layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Re-arrange mask to flatten shape\n",
    "        reshaped_mask = layer_masks[layer_name][mask_index].view(num_heads * 3 * head_dim)\n",
    "        rehsaped_indices = layer_masks[layer_name][sorted_weight_index].view(num_heads * 3 * head_dim)\n",
    "        num_nodes_to_drop = int(sum(reshaped_mask).item())\n",
    "        keep_index = torch.sort(rehsaped_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        # Prune as output\n",
    "        prev_layer_name = layer_name.replace(\"query_key_value_fused_output\", \"query_key_value\")\n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "        \n",
    "    else:\n",
    "        # Prune when nodes are the input\n",
    "        num_nodes_to_drop = int(sum(layer_masks[layer_name][mask_index]).item())\n",
    "        keep_index = torch.sort(layer_masks[layer_name][sorted_weight_index][num_nodes_to_drop:]).values\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[layer_name], -1, keep_index)\n",
    "        pruned_model_params[layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Go to previous layer and prune when nodes are the output\n",
    "        prev_layer_index = params_to_index[layer_name] - 1\n",
    "        prev_layer_name = index_to_params[prev_layer_index]\n",
    "        \n",
    "        if \"layernorm\" in prev_layer_name:\n",
    "            pruned_layer_norm_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "            pruned_model_params[prev_layer_name] = pruned_layer_norm_weights \n",
    "            \n",
    "            # Also do bias term\n",
    "            bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "            pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "            pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "            \n",
    "            prev_layer_index = prev_layer_index - 1\n",
    "            prev_layer_name = index_to_params[prev_layer_index]\n",
    "            \n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdb4a783-6cd1-44f9-b4e9-a3dd2f5b0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model_params, \"pruned_560m_bloom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b70a7e7-b902-4ecd-8c82-bb0764f3b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ae74350-2038-435f-b708-e38f0cdee675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight torch.Size([250880, 1024])\n",
      "transformer.word_embeddings_layernorm.weight torch.Size([1024])\n",
      "transformer.word_embeddings_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.0.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.Size([2197, 1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias torch.Size([2197])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.Size([1024, 2197])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.Size([2352, 1024])\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.Size([2352])\n",
      "transformer.h.1.self_attention.dense.weight torch.Size([1024, 784])\n",
      "transformer.h.1.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.Size([2499, 1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias torch.Size([2499])\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.Size([1024, 2499])\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.Size([3024, 1024])\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.Size([3024])\n",
      "transformer.h.2.self_attention.dense.weight torch.Size([1024, 1008])\n",
      "transformer.h.2.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.Size([2451, 1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias torch.Size([2451])\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.Size([1024, 2451])\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.Size([2928, 1024])\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.Size([2928])\n",
      "transformer.h.3.self_attention.dense.weight torch.Size([1024, 976])\n",
      "transformer.h.3.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.Size([2286, 1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias torch.Size([2286])\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.Size([1024, 2286])\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.Size([2784, 1024])\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.Size([2784])\n",
      "transformer.h.4.self_attention.dense.weight torch.Size([1024, 928])\n",
      "transformer.h.4.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.Size([2880, 1024])\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.Size([2880])\n",
      "transformer.h.5.self_attention.dense.weight torch.Size([1024, 960])\n",
      "transformer.h.5.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.Size([2245, 1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias torch.Size([2245])\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.Size([1024, 2245])\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.6.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.Size([2229, 1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias torch.Size([2229])\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.Size([1024, 2229])\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.7.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.Size([2592, 1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias torch.Size([2592])\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.Size([1024, 2592])\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.8.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.Size([1587, 1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias torch.Size([1587])\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.Size([1024, 1587])\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.9.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.9.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.Size([1031, 1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias torch.Size([1031])\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.Size([1024, 1031])\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.10.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.Size([1448, 1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias torch.Size([1448])\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.Size([1024, 1448])\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.11.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.11.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.Size([2185, 1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias torch.Size([2185])\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.Size([1024, 2185])\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.12.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.12.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.Size([1866, 1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias torch.Size([1866])\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.Size([1024, 1866])\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.13.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.Size([1007, 1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias torch.Size([1007])\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.Size([1024, 1007])\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.14.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.Size([3053, 1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias torch.Size([3053])\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.Size([1024, 3053])\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.Size([3024, 1024])\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.Size([3024])\n",
      "transformer.h.15.self_attention.dense.weight torch.Size([1024, 1008])\n",
      "transformer.h.15.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.Size([211, 1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias torch.Size([211])\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.Size([1024, 211])\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.Size([3024, 1024])\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.Size([3024])\n",
      "transformer.h.16.self_attention.dense.weight torch.Size([1024, 1008])\n",
      "transformer.h.16.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.Size([3133, 1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias torch.Size([3133])\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.Size([1024, 3133])\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.Size([3024, 1024])\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.Size([3024])\n",
      "transformer.h.17.self_attention.dense.weight torch.Size([1024, 1008])\n",
      "transformer.h.17.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.Size([3077, 1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias torch.Size([3077])\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.Size([1024, 3077])\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.18.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.18.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.Size([3751, 1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias torch.Size([3751])\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.Size([1024, 3751])\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.Size([2928, 1024])\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.Size([2928])\n",
      "transformer.h.19.self_attention.dense.weight torch.Size([1024, 976])\n",
      "transformer.h.19.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.Size([2771, 1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias torch.Size([2771])\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.Size([1024, 2771])\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.Size([2832, 1024])\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.Size([2832])\n",
      "transformer.h.20.self_attention.dense.weight torch.Size([1024, 944])\n",
      "transformer.h.20.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.Size([3819, 1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias torch.Size([3819])\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.Size([1024, 3819])\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.Size([2784, 1024])\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.Size([2784])\n",
      "transformer.h.21.self_attention.dense.weight torch.Size([1024, 928])\n",
      "transformer.h.21.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.Size([3980, 1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias torch.Size([3980])\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.Size([1024, 3980])\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.Size([2112, 1024])\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.Size([2112])\n",
      "transformer.h.22.self_attention.dense.weight torch.Size([1024, 704])\n",
      "transformer.h.22.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.Size([4086, 1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias torch.Size([4086])\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.Size([1024, 4086])\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.Size([3024, 1024])\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.Size([3024])\n",
      "transformer.h.23.self_attention.dense.weight torch.Size([1024, 1008])\n",
      "transformer.h.23.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.Size([4070, 1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias torch.Size([4070])\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.Size([1024, 4070])\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.weight torch.Size([250880, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pruned_model_params.keys():\n",
    "    state_dict_shapes[param_name] = pruned_model_params[param_name].shape\n",
    "    print(param_name, pruned_model_params[param_name].shape)\n",
    "    \n",
    "pkl.dump(state_dict_shapes, open(\"state_dict_shapes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28af6ba2-5c54-4693-a1ba-56cc2d8da64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1614772432572614"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num / len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81c1d532-4970-4891-88a0-b1110c867364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39850"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd1a494f-b336-4170-94b7-a230dc8c1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.4.mlp.dense_4h_to_h.weight\n",
      "lm_head.weight\n",
      "transformer.h.6.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.14.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.0.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.9.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.13.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.7.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.11.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.18.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.8.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.12.self_attention.query_key_value_fused_output.weight\n",
      "transformer.h.10.self_attention.query_key_value_fused_output.weight\n"
     ]
    }
   ],
   "source": [
    "for layer_name, _ in all_layers:\n",
    "    if layer_name not in layer_masks:\n",
    "        print(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05bdc349-a599-4c4d-ac35-c6699946a34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        continue\n",
    "\n",
    "    # print(name, param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tap",
   "language": "python",
   "name": "tap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
