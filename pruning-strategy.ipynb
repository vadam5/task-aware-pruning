{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec25fe-bed3-43a9-94e8-22413302a7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from node_attribution.gradient_node_attribution import get_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289d2a47-0d93-43ad-897b-3ae05dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"560m\"\n",
    "data = [\"Oh, interesting, I am not familiar with that movie! Can you tell me more about it?\"]\n",
    "prune_percent = 0.10\n",
    "num_params_to_prune = 559214592 * prune_percent\n",
    "\n",
    "head_dim = 64\n",
    "num_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8efbe0a3-048c-4ca6-a77e-dfe9315161ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading bigscience/bloom-560m.\n",
      "                    Num Transformer Blocks: 24\n",
      "                    Num Attention Heads: 16\n",
      "                    Head Dim: 64\n",
      "                    Hidden Size: 1024\n",
      "                    Base Model Param Count: 559,214,592\n",
      "                    Total Param Count (w/ LM Head): 816,115,712\n"
     ]
    }
   ],
   "source": [
    "# Get attributions\n",
    "avg_contributions, max_contributions, model, model_params = get_attributions(model_size, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b7a08a-65d7-4158-9e60-fd6839f59c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take absolute value of average contributions\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    avg_contributions[layer_name] = torch.abs(avg_contributions[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e880a64-b350-4423-9298-5ef32f86976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "params_to_index = {}\n",
    "for param_name in model_params.keys():\n",
    "    if \"bias\" not in param_name:\n",
    "        params_to_index[param_name] = index\n",
    "        index += 1\n",
    "    \n",
    "index_to_params = {params_to_index[param_name]: param_name for param_name in params_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830cf301-0660-43f4-839c-02016f3bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shape_map = {}\n",
    "layer_names, contribution_tensors = zip(*avg_contributions.items())\n",
    "num_layers = len(layer_names)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer_name = layer_names[i]\n",
    "    layer_size = contribution_tensors[i].shape[0]\n",
    "    \n",
    "    if i != 0:\n",
    "        next_layer_size = contribution_tensors[i - 1].shape[0]\n",
    "    else:\n",
    "        next_layer_size = 250880\n",
    "        \n",
    "    \n",
    "    if i != (num_layers - 1):\n",
    "        prev_layer_size = contribution_tensors[i + 1].shape[0]\n",
    "    else:\n",
    "        prev_layer_size = 1024\n",
    "        \n",
    "    layer_shape_map[layer_name] = {\n",
    "        \"prev_layer_size\": prev_layer_size,\n",
    "        \"next_layer_size\": next_layer_size,\n",
    "        \"current_layer_size\": layer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52304b74-b166-4c13-a95f-2eafc0c3a705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    for node_id, node in enumerate(contribution_tensor.tolist()):\n",
    "        node_name = f\"{layer_name}.{node_id}\"\n",
    "        all_nodes.append((node_name, node))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee0d3a6-0b98-4e18-9940-34b0f3ed7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all the nodes\n",
    "all_nodes.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7061e785-c991-4e60-b4c3-80bfb778cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = []\n",
    "\n",
    "# Figure out which layers have the most to prune\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    \n",
    "    # Don't prune self_attention.dense.weight directly, use value matrix to decide what to prune\n",
    "    if \"self_attention.dense.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"mlp.dense_h_to_4h.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"self_attention.query_key_value.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"transformer.ln_f.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"lm_head.weight\" in layer_name:\n",
    "        continue\n",
    "    \n",
    "    # Get average contribution over the whole layer\n",
    "    mean_contribution = torch.mean(contribution_tensor, 0).item()\n",
    "    all_layers.append((layer_name, mean_contribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6568baf2-3382-4e41-a309-e20b827d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f75b8e-7803-464c-b368-21c453ba0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_masks = {}\n",
    "num_params_pruned = 0\n",
    "node_num = 0\n",
    "min_nodes = 24\n",
    "value_dim = 2\n",
    "\n",
    "while num_params_pruned < num_params_to_prune:\n",
    "    lowest_contr_layer_name = all_layers[0][0]               \n",
    "    shapes = layer_shape_map[lowest_contr_layer_name] \n",
    "    stop_pruning_layer = False\n",
    " \n",
    "    # Prune one node at time\n",
    "    if lowest_contr_layer_name not in layer_masks:\n",
    "        layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "        mask = torch.zeros_like(layer_contributions)\n",
    "        sorted_contributions = torch.argsort(layer_contributions)\n",
    "        num_pruned = 0\n",
    "\n",
    "    else:\n",
    "        mask = layer_masks[lowest_contr_layer_name][0]\n",
    "        sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "        num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "    index_to_mask = sorted_contributions[num_pruned]\n",
    "    mask[index_to_mask] = 1\n",
    "\n",
    "    nodes_left = torch.numel(mask) - int(torch.sum(mask).item())\n",
    "\n",
    "    # Keep from deleting all nodes in a layer\n",
    "    if nodes_left > min_nodes:\n",
    "        layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "        # num_params_pruned += shapes[\"prev_layer_size\"]\n",
    "        # num_params_pruned += shapes[\"next_layer_size\"]\n",
    "        num_params_pruned += 2048\n",
    "        node_num += 1\n",
    "    else:\n",
    "        stop_pruning_layer = True\n",
    "\n",
    "    # Apply mask and update the layer mean in \"all_layers\"\n",
    "    if stop_pruning_layer:\n",
    "        new_layer_contr_score = float('inf')\n",
    "    else:\n",
    "        mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=mask)\n",
    "        new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    # print(all_layers[0])\n",
    "    all_layers[0] = (lowest_contr_layer_name, new_layer_contr_score)\n",
    "    # print(all_layers[0])\n",
    "    # print(f\"Num params removed: {num_params_pruned}\")\n",
    "    # print(f\"Num Nodes removed: {node_num}\")\n",
    "    # print(\"=====\")\n",
    "    \n",
    "    # re-sort layers now that this one has been pruned and pick the lowest contributing layer again\n",
    "    all_layers.sort(key = lambda x:x[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ec7830-fe51-4256-bbe6-2412d244a3b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.h.23.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([1948, 2158, 3014,  ..., 3193, 3253,  353]),\n",
       "  4027),\n",
       " 'transformer.h.22.mlp.dense_4h_to_h.weight': (tensor([1., 0., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([3998, 3586,  469,  ..., 3428,  434, 2701]),\n",
       "  4001),\n",
       " 'transformer.h.21.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([  74, 2842, 3858,  ..., 1575,  813,  973]),\n",
       "  3708),\n",
       " 'transformer.h.9.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 0., 0., 1.]),\n",
       "  tensor([1413, 2233, 3739,  ..., 2755, 1009, 1599]),\n",
       "  2454),\n",
       " 'transformer.h.20.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 0., 1.]),\n",
       "  tensor([3903, 2849,  534,  ..., 2642, 2293, 1089]),\n",
       "  3035),\n",
       " 'transformer.h.8.mlp.dense_4h_to_h.weight': (tensor([0., 1., 1.,  ..., 0., 0., 1.]),\n",
       "  tensor([ 376, 2034,  153,  ..., 2904,   67,  846]),\n",
       "  2264),\n",
       " 'transformer.h.7.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 1., 0., 0.]),\n",
       "  tensor([2054, 2567, 1038,  ..., 2639, 1237, 1911]),\n",
       "  1693),\n",
       " 'transformer.h.19.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 0., 1., 1.]),\n",
       "  tensor([2903, 3698, 1746,  ...,  849,  540, 1058]),\n",
       "  1650),\n",
       " 'transformer.h.10.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([1355, 2639, 1441,  ..., 1275,  162, 3211]),\n",
       "  755),\n",
       " 'transformer.h.6.mlp.dense_4h_to_h.weight': (tensor([0., 0., 1.,  ..., 0., 1., 1.]),\n",
       "  tensor([2761, 2553, 1269,  ...,  818, 3450,  308]),\n",
       "  767),\n",
       " 'transformer.h.18.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([2730, 2080, 2407,  ..., 1973, 3658,  556]),\n",
       "  732),\n",
       " 'transformer.h.5.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 1., 0.]),\n",
       "  tensor([2065,  635, 3538,  ..., 3526,  326,  477]),\n",
       "  663),\n",
       " 'transformer.h.11.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([3671, 3394,  306,  ..., 3925, 3932, 2145]),\n",
       "  440),\n",
       " 'transformer.h.13.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([ 737, 2569, 2488,  ..., 1565, 2399, 1535]),\n",
       "  420),\n",
       " 'transformer.h.14.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       "  tensor([3509, 3018,  793,  ..., 4002, 1260,  828]),\n",
       "  419),\n",
       " 'transformer.h.4.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([3998, 2786, 2582,  ..., 3577, 2308,  824]),\n",
       "  155),\n",
       " 'transformer.h.15.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([1533, 2382, 3604,  ...,  932, 2129,  545]),\n",
       "  69),\n",
       " 'transformer.h.17.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([1793, 1998, 1419,  ..., 1186, 2584,  411]),\n",
       "  38),\n",
       " 'transformer.h.16.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([ 218,  962,  132,  ...,  119, 1300, 2847]),\n",
       "  16)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0d48da-8f45-4f78-a502-ed4e413b6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Line up weights to prune and weights in the state dict\n",
    "mask_index = 0\n",
    "sorted_weight_index = 1\n",
    "pruned_model_params = model_params.copy()\n",
    "\n",
    "for layer_name in layer_masks.keys():\n",
    "    if layer_name == \"transformer.h.0.self_attention.query_key_value.weight\":\n",
    "        continue\n",
    "    elif \"self_attention.value_layer.weight\" in layer_name:   \n",
    "        # Prune as input\n",
    "        # Look at value matrix to decide what should be droped in \"self_attention.dense.weight\"\n",
    "        value_reshape_mask = layer_masks[layer_name][mask_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        num_nodes_to_drop = int(sum(value_reshape_mask).item())\n",
    "        value_indices = layer_masks[layer_name][sorted_weight_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        value_keep_index = torch.sort(value_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        dense_layer_name = layer_name.replace(\"value_layer\", \"dense\")\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[dense_layer_name], -1, value_keep_index)\n",
    "        pruned_model_params[dense_layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Re-arrange mask to flatten shape\n",
    "        reshaped_mask = layer_masks[layer_name][mask_index].view(num_heads * 3 * head_dim)\n",
    "        rehsaped_indices = layer_masks[layer_name][sorted_weight_index].view(num_heads * 3 * head_dim)\n",
    "        num_nodes_to_drop = int(sum(reshaped_mask).item())\n",
    "        keep_index = torch.sort(rehsaped_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        # Prune as output\n",
    "        prev_layer_name = layer_name.replace(\"value_layer\", \"query_key_value\")\n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "        \n",
    "    else:\n",
    "        # Prune when nodes are the input\n",
    "        num_nodes_to_drop = int(sum(layer_masks[layer_name][mask_index]).item())\n",
    "        keep_index = torch.sort(layer_masks[layer_name][sorted_weight_index][num_nodes_to_drop:]).values\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[layer_name], -1, keep_index)\n",
    "        pruned_model_params[layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Go to previous layer and prune when nodes are the output\n",
    "        prev_layer_index = params_to_index[layer_name] - 1\n",
    "        prev_layer_name = index_to_params[prev_layer_index]\n",
    "        \n",
    "        if \"layernorm\" in prev_layer_name:\n",
    "            pruned_layer_norm_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "            pruned_model_params[prev_layer_name] = pruned_layer_norm_weights \n",
    "            \n",
    "            # Also do bias term\n",
    "            bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "            pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "            pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "            \n",
    "            prev_layer_index = prev_layer_index - 1\n",
    "            prev_layer_name = index_to_params[prev_layer_index]\n",
    "            \n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb4a783-6cd1-44f9-b4e9-a3dd2f5b0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model_params, \"pruned_560m_bloom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b70a7e7-b902-4ecd-8c82-bb0764f3b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae74350-2038-435f-b708-e38f0cdee675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight torch.Size([250880, 1024])\n",
      "transformer.word_embeddings_layernorm.weight torch.Size([1024])\n",
      "transformer.word_embeddings_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.0.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.1.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.1.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.2.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.2.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.3.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.3.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.4.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.4.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.Size([3941, 1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias torch.Size([3941])\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.Size([1024, 3941])\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.5.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.5.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.Size([3433, 1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias torch.Size([3433])\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.Size([1024, 3433])\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.6.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.Size([3329, 1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias torch.Size([3329])\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.Size([1024, 3329])\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.7.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.Size([2403, 1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias torch.Size([2403])\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.Size([1024, 2403])\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.8.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.Size([1832, 1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias torch.Size([1832])\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.Size([1024, 1832])\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.9.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.9.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.Size([1642, 1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias torch.Size([1642])\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.Size([1024, 1642])\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.10.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.Size([3341, 1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias torch.Size([3341])\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.Size([1024, 3341])\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.11.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.11.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.Size([3656, 1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias torch.Size([3656])\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.Size([1024, 3656])\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.12.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.12.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.13.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.Size([3676, 1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias torch.Size([3676])\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.Size([1024, 3676])\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.14.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.Size([3677, 1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias torch.Size([3677])\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.Size([1024, 3677])\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.15.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.15.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.Size([4027, 1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias torch.Size([4027])\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.Size([1024, 4027])\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.16.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.16.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.Size([4080, 1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias torch.Size([4080])\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.Size([1024, 4080])\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.17.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.17.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.Size([4058, 1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias torch.Size([4058])\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.Size([1024, 4058])\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.18.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.18.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.Size([3364, 1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias torch.Size([3364])\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.Size([1024, 3364])\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.19.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.19.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.Size([2446, 1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias torch.Size([2446])\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.Size([1024, 2446])\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.20.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.20.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.Size([1061, 1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias torch.Size([1061])\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.Size([1024, 1061])\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.21.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.21.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.Size([388, 1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias torch.Size([388])\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.Size([1024, 388])\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.22.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.22.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.Size([95, 1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias torch.Size([95])\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.Size([1024, 95])\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.23.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.23.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.Size([69, 1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias torch.Size([69])\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.Size([1024, 69])\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.weight torch.Size([250880, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pruned_model_params.keys():\n",
    "    state_dict_shapes[param_name] = pruned_model_params[param_name].shape\n",
    "    print(param_name, pruned_model_params[param_name].shape)\n",
    "    \n",
    "pkl.dump(state_dict_shapes, open(\"state_dict_shapes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28af6ba2-5c54-4693-a1ba-56cc2d8da64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27777099609375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num / len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c1d532-4970-4891-88a0-b1110c867364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27306"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeed23-709d-4bc9-b439-e977deb53796",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # If the layer is a query_key_value_fused_output layer\n",
    "#     elif \"query_key_value_fused_output\" in lowest_contr_layer_name:\n",
    "\n",
    "#         # Build pruning mask\n",
    "#         if lowest_contr_layer_name not in layer_masks:\n",
    "#             layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "#             layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "\n",
    "#             mask = torch.zeros_like(layer_contributions)\n",
    "#             sorted_contributions = torch.argsort(layer_contributions)\n",
    "#             st = torch.sort(layer_contributions)\n",
    "#             num_pruned = 0\n",
    "#         else:\n",
    "#             mask = layer_masks[lowest_contr_layer_name][0]\n",
    "#             sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "#             num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "#         # Prune 3 * num head nodes at once, (this will decrease head dim by 1 for each atten head in this layer)\n",
    "#         # Make sure each of the 3 nodes are in the 3 different query, key, and value regions\n",
    "\n",
    "#         # TODO: Find a more efficient way of doing this\n",
    "#         for head in range(num_heads):\n",
    "#             for qkv in range(3):\n",
    "#                 index_to_mask = sorted_contributions[head][qkv][num_pruned]\n",
    "#                 mask[head][qkv][index_to_mask] = 1\n",
    "        \n",
    "#         nodes_left = torch.numel(mask) - int(torch.sum(mask, dim=[0, 1, 2]).item())\n",
    "        \n",
    "#         # Keep from deleting all nodes in a layer\n",
    "#         if nodes_left > min_nodes:\n",
    "#             layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "#             num_params_pruned += (1024 * num_heads * 3)\n",
    "#             num_params_pruned += (1024 * num_heads * 3)\n",
    "#             node_num += (num_heads * 3)\n",
    "#         else:\n",
    "#             stop_pruning_layer = True \n",
    "\n",
    "\n",
    "   \n",
    "#     if \"self_attention.value_layer.weight\" in lowest_contr_layer_name:\n",
    "#         if lowest_contr_layer_name not in layer_masks:\n",
    "#             layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "#             qk_place_holder = torch.full((2 * layer_contributions.shape[0],), float('inf'))\n",
    "#             layer_contributions = torch.cat((qk_place_holder, layer_contributions))\n",
    "#             layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "    \n",
    "#             mask = torch.zeros_like(layer_contributions)\n",
    "#             sorted_contributions = torch.argsort(layer_contributions)\n",
    "#             st = torch.sort(layer_contributions)\n",
    "#             num_pruned = 0\n",
    "#         else:\n",
    "#             mask = layer_masks[lowest_contr_layer_name][0]\n",
    "#             sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "#             num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "            \n",
    "#         # TODO: Find a more efficient way of doing this\n",
    "#         for head in range(num_heads):\n",
    "#             index_to_mask = sorted_contributions[head][value_dim][num_pruned]\n",
    "#             mask[head][value_dim][index_to_mask] = 1\n",
    "        \n",
    "#         nodes_left = head_dim - (num_pruned + 1)\n",
    "        \n",
    "#         # Keep from deleting all nodes in a layer\n",
    "#         if nodes_left > min_nodes:\n",
    "#             layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "#             num_params_pruned += (1024 * num_heads)\n",
    "#             num_params_pruned += (1024 * num_heads)\n",
    "#             node_num += num_heads\n",
    "#         else:\n",
    "#             stop_pruning_layer = True\n",
    "            \n",
    "#         # Apply mask and update the layer mean in \"all_layers\"\n",
    "#         if stop_pruning_layer:\n",
    "#             new_layer_contr_score = float('inf')\n",
    "#         else:\n",
    "#             value_mask = torch.index_select(mask, dim=1, index=torch.tensor([2]))\n",
    "#             mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=value_mask)\n",
    "#             new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "#     else:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tap",
   "language": "python",
   "name": "tap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
