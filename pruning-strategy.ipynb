{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec25fe-bed3-43a9-94e8-22413302a7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from node_attribution.gradient_node_attribution import get_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289d2a47-0d93-43ad-897b-3ae05dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"560m\"\n",
    "data = [\"Oh, interesting, I am not familiar with that movie! Can you tell me more about it?\"]\n",
    "prune_percent = 0.10\n",
    "num_params_to_prune = 559214592 * prune_percent\n",
    "\n",
    "head_dim = 64\n",
    "num_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8efbe0a3-048c-4ca6-a77e-dfe9315161ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading bigscience/bloom-560m.\n",
      "                    Num Transformer Blocks: 24\n",
      "                    Num Attention Heads: 16\n",
      "                    Head Dim: 64\n",
      "                    Hidden Size: 1024\n",
      "                    Base Model Param Count: 559,214,592\n",
      "                    Total Param Count (w/ LM Head): 816,115,712\n"
     ]
    }
   ],
   "source": [
    "# Get attributions\n",
    "avg_contributions, max_contributions, model, model_params = get_attributions(model_size, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b7a08a-65d7-4158-9e60-fd6839f59c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take absolute value of average contributions\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    avg_contributions[layer_name] = torch.abs(avg_contributions[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e880a64-b350-4423-9298-5ef32f86976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "params_to_index = {}\n",
    "for param_name in model_params.keys():\n",
    "    if \"bias\" not in param_name:\n",
    "        params_to_index[param_name] = index\n",
    "        index += 1\n",
    "    \n",
    "index_to_params = {params_to_index[param_name]: param_name for param_name in params_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830cf301-0660-43f4-839c-02016f3bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shape_map = {}\n",
    "layer_names, contribution_tensors = zip(*avg_contributions.items())\n",
    "num_layers = len(layer_names)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer_name = layer_names[i]\n",
    "    layer_size = contribution_tensors[i].shape[0]\n",
    "    \n",
    "    if i != 0:\n",
    "        next_layer_size = contribution_tensors[i - 1].shape[0]\n",
    "    else:\n",
    "        next_layer_size = 250880\n",
    "        \n",
    "    \n",
    "    if i != (num_layers - 1):\n",
    "        prev_layer_size = contribution_tensors[i + 1].shape[0]\n",
    "    else:\n",
    "        prev_layer_size = 1024\n",
    "        \n",
    "    layer_shape_map[layer_name] = {\n",
    "        \"prev_layer_size\": prev_layer_size,\n",
    "        \"next_layer_size\": next_layer_size,\n",
    "        \"current_layer_size\": layer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52304b74-b166-4c13-a95f-2eafc0c3a705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    for node_id, node in enumerate(contribution_tensor.tolist()):\n",
    "        node_name = f\"{layer_name}.{node_id}\"\n",
    "        all_nodes.append((node_name, node))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee0d3a6-0b98-4e18-9940-34b0f3ed7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all the nodes\n",
    "all_nodes.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7061e785-c991-4e60-b4c3-80bfb778cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = []\n",
    "\n",
    "# Figure out which layers have the most to prune\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    \n",
    "    # Don't prune self_attention.dense.weight directly, use value matrix to decide what to prune\n",
    "    if \"self_attention.dense.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"mlp.dense_h_to_4h.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"self_attention.query_key_value.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"transformer.ln_f.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"lm_head.weight\" in layer_name:\n",
    "        continue\n",
    "    \n",
    "    # Get average contribution over the whole layer\n",
    "    mean_contribution = torch.mean(contribution_tensor, 0).item()\n",
    "    all_layers.append((layer_name, mean_contribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6568baf2-3382-4e41-a309-e20b827d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f75b8e-7803-464c-b368-21c453ba0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_masks = {}\n",
    "num_params_pruned = 0\n",
    "node_num = 0\n",
    "min_nodes = 24\n",
    "value_dim = 2\n",
    "\n",
    "while num_params_pruned < num_params_to_prune:\n",
    "    lowest_contr_layer_name = all_layers[0][0]               \n",
    "    shapes = layer_shape_map[lowest_contr_layer_name] \n",
    "    stop_pruning_layer = False\n",
    "    \n",
    "    if \"self_attention.value_layer.weight\" in lowest_contr_layer_name:\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "            qk_place_holder = torch.full((2 * layer_contributions.shape[0],), float('inf'))\n",
    "            layer_contributions = torch.cat((qk_place_holder, layer_contributions))\n",
    "            layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "    \n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            st = torch.sort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "            \n",
    "        # TODO: Find a more efficient way of doing this\n",
    "        for head in range(num_heads):\n",
    "            index_to_mask = sorted_contributions[head][value_dim][num_pruned]\n",
    "            mask[head][value_dim][index_to_mask] = 1\n",
    "        \n",
    "        nodes_left = head_dim - (num_pruned + 1)\n",
    "        \n",
    "        # Keep from deleting all nodes in a layer\n",
    "        if nodes_left > min_nodes:\n",
    "            layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "            num_params_pruned += (1024 * num_heads)\n",
    "            num_params_pruned += (1024 * num_heads)\n",
    "            node_num += num_heads\n",
    "        else:\n",
    "            stop_pruning_layer = True\n",
    "            \n",
    "        # Apply mask and update the layer mean in \"all_layers\"\n",
    "        if stop_pruning_layer:\n",
    "            new_layer_contr_score = float('inf')\n",
    "        else:\n",
    "            value_mask = torch.index_select(mask, dim=1, index=torch.tensor([2]))\n",
    "            mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=value_mask)\n",
    "            new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    else:\n",
    "        # Prune one node at time\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "            \n",
    "        index_to_mask = sorted_contributions[num_pruned]\n",
    "        mask[index_to_mask] = 1\n",
    "        \n",
    "        nodes_left = torch.numel(mask) - int(torch.sum(mask).item())\n",
    "        \n",
    "        # Keep from deleting all nodes in a layer\n",
    "        if nodes_left > min_nodes:\n",
    "            layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "            # num_params_pruned += shapes[\"prev_layer_size\"]\n",
    "            # num_params_pruned += shapes[\"next_layer_size\"]\n",
    "            num_params_pruned += 2048\n",
    "            node_num += 1\n",
    "        else:\n",
    "            stop_pruning_layer = True\n",
    "\n",
    "        # Apply mask and update the layer mean in \"all_layers\"\n",
    "        if stop_pruning_layer:\n",
    "            new_layer_contr_score = float('inf')\n",
    "        else:\n",
    "            mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=mask)\n",
    "            new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    # print(all_layers[0])\n",
    "    all_layers[0] = (lowest_contr_layer_name, new_layer_contr_score)\n",
    "    # print(all_layers[0])\n",
    "    # print(f\"Num params removed: {num_params_pruned}\")\n",
    "    # print(f\"Num Nodes removed: {node_num}\")\n",
    "    # print(\"=====\")\n",
    "    \n",
    "    # re-sort layers now that this one has been pruned and pick the lowest contributing layer again\n",
    "    all_layers.sort(key = lambda x:x[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ec7830-fe51-4256-bbe6-2412d244a3b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.h.6.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       "  tensor([2761, 2553, 1269,  ...,  818, 3450,  308]),\n",
       "  319),\n",
       " 'transformer.h.18.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([2730, 2080, 2407,  ..., 1973, 3658,  556]),\n",
       "  279),\n",
       " 'transformer.h.20.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 0., 1.]),\n",
       "  tensor([3903, 2849,  534,  ..., 2642, 2293, 1089]),\n",
       "  2774),\n",
       " 'transformer.h.7.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 1., 0., 0.]),\n",
       "  tensor([2054, 2567, 1038,  ..., 2639, 1237, 1911]),\n",
       "  1379),\n",
       " 'transformer.h.9.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 0., 0., 1.]),\n",
       "  tensor([1413, 2233, 3739,  ..., 2755, 1009, 1599]),\n",
       "  2246),\n",
       " 'transformer.h.19.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 0., 1., 1.]),\n",
       "  tensor([2903, 3698, 1746,  ...,  849,  540, 1058]),\n",
       "  1219),\n",
       " 'transformer.h.8.mlp.dense_4h_to_h.weight': (tensor([0., 0., 1.,  ..., 0., 0., 0.]),\n",
       "  tensor([ 376, 2034,  153,  ..., 2904,   67,  846]),\n",
       "  2028),\n",
       " 'transformer.h.10.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([1355, 2639, 1441,  ..., 1275,  162, 3211]),\n",
       "  353),\n",
       " 'transformer.h.21.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([  74, 2842, 3858,  ..., 1575,  813,  973]),\n",
       "  3615),\n",
       " 'transformer.h.17.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 1.,  ..., 1., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 1., 1., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[32, 63, 25,  ...,  8,  0, 39],\n",
       "           [ 5, 44, 42,  ..., 54, 59, 21],\n",
       "           [26, 35, 39,  ..., 41, 15,  0]],\n",
       "  \n",
       "          [[ 2, 28, 34,  ...,  9, 43, 22],\n",
       "           [57, 51, 61,  ..., 55,  7, 37],\n",
       "           [59, 34, 14,  ..., 20,  5, 21]],\n",
       "  \n",
       "          [[16, 36, 15,  ...,  4, 53, 20],\n",
       "           [36,  7, 16,  ..., 15, 26, 23],\n",
       "           [29, 14, 59,  ...,  4,  0, 40]]]),\n",
       "  34),\n",
       " 'transformer.h.2.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 1., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 1., 1., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[59,  2, 57,  ..., 13, 14, 32],\n",
       "           [ 6, 18,  4,  ..., 30, 38, 22],\n",
       "           [61,  8, 24,  ..., 58, 54, 26]],\n",
       "  \n",
       "          [[38, 12, 13,  ..., 31, 62, 41],\n",
       "           [48, 23, 20,  ...,  5, 35, 46],\n",
       "           [14, 11,  9,  ...,  0, 62, 48]],\n",
       "  \n",
       "          [[31, 54, 58,  ..., 22, 21, 62],\n",
       "           [18, 63, 53,  ..., 14, 34, 56],\n",
       "           [60, 29, 58,  ..., 52, 23, 43]]]),\n",
       "  39),\n",
       " 'transformer.h.18.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 1.,  ..., 0., 1., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 1.,  ..., 1., 1., 0.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[60, 18, 50,  ..., 62, 59, 53],\n",
       "           [29, 11,  1,  ..., 37, 52, 18],\n",
       "           [35, 34, 45,  ..., 20, 51,  0]],\n",
       "  \n",
       "          [[49,  3, 62,  ...,  6, 16, 59],\n",
       "           [50, 19, 28,  ..., 21,  9, 47],\n",
       "           [23, 38,  1,  ...,  2,  9, 52]],\n",
       "  \n",
       "          [[ 1, 41, 51,  ..., 50, 28, 44],\n",
       "           [45,  1, 22,  ..., 53, 10, 48],\n",
       "           [57, 44, 48,  ..., 52, 59, 46]]]),\n",
       "  32),\n",
       " 'transformer.h.1.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 0., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 1., 1., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[24, 18, 54,  ..., 20, 46, 21],\n",
       "           [59, 28, 29,  ...,  6, 20, 36],\n",
       "           [25, 30, 43,  ..., 50,  3, 47]],\n",
       "  \n",
       "          [[50, 47, 29,  ..., 21, 20, 12],\n",
       "           [51, 29,  0,  ..., 61, 59, 32],\n",
       "           [46, 60, 14,  ...,  4, 18,  1]],\n",
       "  \n",
       "          [[10, 56, 36,  ..., 17, 40, 15],\n",
       "           [32, 20,  3,  ..., 12,  2, 36],\n",
       "           [55, 62,  4,  ..., 46, 42, 26]]]),\n",
       "  39),\n",
       " 'transformer.h.11.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 1., 1., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 1., 1., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 0., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[59, 13, 37,  ..., 14,  3, 19],\n",
       "           [22, 53, 44,  ..., 31, 20, 32],\n",
       "           [45, 54, 27,  ...,  3, 55, 47]],\n",
       "  \n",
       "          [[45, 40, 49,  ..., 18,  0, 30],\n",
       "           [22, 10, 45,  ..., 24, 25, 27],\n",
       "           [36, 35,  8,  ..., 43,  5, 30]],\n",
       "  \n",
       "          [[42, 32, 38,  ..., 17, 16, 23],\n",
       "           [30, 15, 27,  ..., 20, 22,  7],\n",
       "           [ 4, 33, 24,  ..., 40, 16, 10]]]),\n",
       "  39),\n",
       " 'transformer.h.22.mlp.dense_4h_to_h.weight': (tensor([1., 0., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([3998, 3586,  469,  ..., 3428,  434, 2701]),\n",
       "  3980),\n",
       " 'transformer.h.12.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 1.,  ..., 0., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 1., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 1., 1., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[57, 44, 54,  ..., 48, 29, 61],\n",
       "           [15, 47, 37,  ..., 51, 33,  5],\n",
       "           [15, 19,  4,  ..., 20, 55, 36]],\n",
       "  \n",
       "          [[47, 54, 21,  ..., 51, 18,  9],\n",
       "           [23, 57, 55,  ..., 21, 48, 60],\n",
       "           [38, 34,  3,  ..., 62,  2, 31]],\n",
       "  \n",
       "          [[49, 56, 33,  ..., 41, 50, 35],\n",
       "           [55, 36, 46,  ..., 24,  3, 17],\n",
       "           [60, 57, 32,  ..., 31, 52, 30]]]),\n",
       "  38),\n",
       " 'transformer.h.23.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "  tensor([1948, 2158, 3014,  ..., 3193, 3253,  353]),\n",
       "  4013),\n",
       " 'transformer.h.9.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 0.,  ..., 1., 1., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 0.,  ..., 1., 1., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[40, 39, 27,  ..., 48, 43, 52],\n",
       "           [48, 42, 33,  ..., 43, 40, 49],\n",
       "           [31, 59, 20,  ..., 55, 15, 36]],\n",
       "  \n",
       "          [[11, 30, 31,  ..., 51, 15, 25],\n",
       "           [10, 54, 41,  ..., 29,  6, 53],\n",
       "           [10, 47,  9,  ...,  0, 34,  8]],\n",
       "  \n",
       "          [[39, 43,  7,  ...,  0, 44, 41],\n",
       "           [16,  5, 62,  ...,  1, 44, 60],\n",
       "           [ 3, 61, 29,  ..., 35, 22,  2]]]),\n",
       "  39),\n",
       " 'transformer.h.16.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 0.,  ..., 0., 1., 1.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[24, 28, 32,  ..., 49,  7, 39],\n",
       "           [45, 42, 56,  ..., 36, 46, 43],\n",
       "           [23,  1,  3,  ..., 53, 27, 35]],\n",
       "  \n",
       "          [[45,  7, 16,  ..., 17, 35, 20],\n",
       "           [31, 35, 21,  ..., 57, 16, 26],\n",
       "           [ 6, 35,  3,  ..., 10, 47, 42]],\n",
       "  \n",
       "          [[ 5, 43, 34,  ..., 26, 40, 13],\n",
       "           [12, 14, 59,  ..., 62, 44,  9],\n",
       "           [36, 62, 43,  ..., 39, 47, 54]]]),\n",
       "  23),\n",
       " 'transformer.h.5.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 1., 0.]),\n",
       "  tensor([2065,  635, 3538,  ..., 3526,  326,  477]),\n",
       "  233),\n",
       " 'transformer.h.15.self_attention.value_layer.weight': (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 1., 0., 0.]]]),\n",
       "  tensor([[[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          [[ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63],\n",
       "           [ 0,  1,  2,  ..., 61, 62, 63]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 9, 37,  0,  ..., 31, 28, 61],\n",
       "           [61, 20,  5,  ..., 57,  3, 33],\n",
       "           [15, 33, 43,  ..., 26, 27, 60]],\n",
       "  \n",
       "          [[ 4, 18, 44,  ...,  7, 46, 22],\n",
       "           [54, 35, 32,  ...,  6, 44, 31],\n",
       "           [18, 38, 31,  ..., 50, 58, 47]],\n",
       "  \n",
       "          [[13,  5, 11,  ..., 23, 45,  7],\n",
       "           [35, 36, 48,  ..., 50,  4,  1],\n",
       "           [60, 45, 35,  ..., 47, 41, 17]]]),\n",
       "  21),\n",
       " 'transformer.h.11.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([3671, 3394,  306,  ..., 3925, 3932, 2145]),\n",
       "  9)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0d48da-8f45-4f78-a502-ed4e413b6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Line up weights to prune and weights in the state dict\n",
    "mask_index = 0\n",
    "sorted_weight_index = 1\n",
    "pruned_model_params = model_params.copy()\n",
    "\n",
    "for layer_name in layer_masks.keys():\n",
    "    if layer_name == \"transformer.h.0.self_attention.query_key_value.weight\":\n",
    "        continue\n",
    "    elif \"self_attention.value_layer.weight\" in layer_name:   \n",
    "        # Prune as input\n",
    "        # Look at value matrix to decide what should be droped in \"self_attention.dense.weight\"\n",
    "        value_reshape_mask = layer_masks[layer_name][mask_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        num_nodes_to_drop = int(sum(value_reshape_mask).item())\n",
    "        value_indices = layer_masks[layer_name][sorted_weight_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        value_keep_index = torch.sort(value_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        dense_layer_name = layer_name.replace(\"value_layer\", \"dense\")\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[dense_layer_name], -1, value_keep_index)\n",
    "        pruned_model_params[dense_layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Re-arrange mask to flatten shape\n",
    "        reshaped_mask = layer_masks[layer_name][mask_index].view(num_heads * 3 * head_dim)\n",
    "        rehsaped_indices = layer_masks[layer_name][sorted_weight_index].view(num_heads * 3 * head_dim)\n",
    "        num_nodes_to_drop = int(sum(reshaped_mask).item())\n",
    "        keep_index = torch.sort(rehsaped_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        # Prune as output\n",
    "        prev_layer_name = layer_name.replace(\"value_layer\", \"query_key_value\")\n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "        \n",
    "    else:\n",
    "        # Prune when nodes are the input\n",
    "        num_nodes_to_drop = int(sum(layer_masks[layer_name][mask_index]).item())\n",
    "        keep_index = torch.sort(layer_masks[layer_name][sorted_weight_index][num_nodes_to_drop:]).values\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[layer_name], -1, keep_index)\n",
    "        pruned_model_params[layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Go to previous layer and prune when nodes are the output\n",
    "        prev_layer_index = params_to_index[layer_name] - 1\n",
    "        prev_layer_name = index_to_params[prev_layer_index]\n",
    "        \n",
    "        if \"layernorm\" in prev_layer_name:\n",
    "            pruned_layer_norm_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "            pruned_model_params[prev_layer_name] = pruned_layer_norm_weights \n",
    "            \n",
    "            # Also do bias term\n",
    "            bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "            pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "            pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "            \n",
    "            prev_layer_index = prev_layer_index - 1\n",
    "            prev_layer_name = index_to_params[prev_layer_index]\n",
    "            \n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb4a783-6cd1-44f9-b4e9-a3dd2f5b0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model_params, \"pruned_560m_bloom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b70a7e7-b902-4ecd-8c82-bb0764f3b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ae74350-2038-435f-b708-e38f0cdee675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight torch.Size([250880, 1024])\n",
      "transformer.word_embeddings_layernorm.weight torch.Size([1024])\n",
      "transformer.word_embeddings_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.0.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.Size([2432, 1024])\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.Size([2432])\n",
      "transformer.h.1.self_attention.dense.weight torch.Size([1024, 384])\n",
      "transformer.h.1.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.Size([2432, 1024])\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.Size([2432])\n",
      "transformer.h.2.self_attention.dense.weight torch.Size([1024, 384])\n",
      "transformer.h.2.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.3.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.3.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.4.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.4.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.5.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.5.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.Size([3863, 1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias torch.Size([3863])\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.Size([1024, 3863])\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.6.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.Size([3777, 1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias torch.Size([3777])\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.Size([1024, 3777])\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.7.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.Size([2717, 1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias torch.Size([2717])\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.Size([1024, 2717])\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.8.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.Size([2068, 1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias torch.Size([2068])\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.Size([1024, 2068])\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.Size([2432, 1024])\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.Size([2432])\n",
      "transformer.h.9.self_attention.dense.weight torch.Size([1024, 384])\n",
      "transformer.h.9.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.Size([1850, 1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias torch.Size([1850])\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.Size([1024, 1850])\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.10.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.Size([3743, 1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias torch.Size([3743])\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.Size([1024, 3743])\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.Size([2432, 1024])\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.Size([2432])\n",
      "transformer.h.11.self_attention.dense.weight torch.Size([1024, 384])\n",
      "transformer.h.11.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.Size([4087, 1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias torch.Size([4087])\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.Size([1024, 4087])\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.Size([2464, 1024])\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.Size([2464])\n",
      "transformer.h.12.self_attention.dense.weight torch.Size([1024, 416])\n",
      "transformer.h.12.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.13.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.14.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.Size([2736, 1024])\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.Size([2736])\n",
      "transformer.h.15.self_attention.dense.weight torch.Size([1024, 688])\n",
      "transformer.h.15.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.Size([2704, 1024])\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.Size([2704])\n",
      "transformer.h.16.self_attention.dense.weight torch.Size([1024, 656])\n",
      "transformer.h.16.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.Size([2528, 1024])\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.Size([2528])\n",
      "transformer.h.17.self_attention.dense.weight torch.Size([1024, 480])\n",
      "transformer.h.17.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.Size([2560, 1024])\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.Size([2560])\n",
      "transformer.h.18.self_attention.dense.weight torch.Size([1024, 512])\n",
      "transformer.h.18.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.Size([3817, 1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias torch.Size([3817])\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.Size([1024, 3817])\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.19.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.19.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.Size([2877, 1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias torch.Size([2877])\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.Size([1024, 2877])\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.20.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.20.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.Size([1322, 1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias torch.Size([1322])\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.Size([1024, 1322])\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.21.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.21.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.Size([481, 1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias torch.Size([481])\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.Size([1024, 481])\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.22.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.22.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.Size([116, 1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias torch.Size([116])\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.Size([1024, 116])\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.23.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.23.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.Size([83, 1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias torch.Size([83])\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.Size([1024, 83])\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.weight torch.Size([250880, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pruned_model_params.keys():\n",
    "    state_dict_shapes[param_name] = pruned_model_params[param_name].shape\n",
    "    print(param_name, pruned_model_params[param_name].shape)\n",
    "    \n",
    "pkl.dump(state_dict_shapes, open(\"state_dict_shapes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28af6ba2-5c54-4693-a1ba-56cc2d8da64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22225748697916667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num / len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c1d532-4970-4891-88a0-b1110c867364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27311"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeed23-709d-4bc9-b439-e977deb53796",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # If the layer is a query_key_value_fused_output layer\n",
    "#     elif \"query_key_value_fused_output\" in lowest_contr_layer_name:\n",
    "\n",
    "#         # Build pruning mask\n",
    "#         if lowest_contr_layer_name not in layer_masks:\n",
    "#             layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "#             layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "\n",
    "#             mask = torch.zeros_like(layer_contributions)\n",
    "#             sorted_contributions = torch.argsort(layer_contributions)\n",
    "#             st = torch.sort(layer_contributions)\n",
    "#             num_pruned = 0\n",
    "#         else:\n",
    "#             mask = layer_masks[lowest_contr_layer_name][0]\n",
    "#             sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "#             num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "#         # Prune 3 * num head nodes at once, (this will decrease head dim by 1 for each atten head in this layer)\n",
    "#         # Make sure each of the 3 nodes are in the 3 different query, key, and value regions\n",
    "\n",
    "#         # TODO: Find a more efficient way of doing this\n",
    "#         for head in range(num_heads):\n",
    "#             for qkv in range(3):\n",
    "#                 index_to_mask = sorted_contributions[head][qkv][num_pruned]\n",
    "#                 mask[head][qkv][index_to_mask] = 1\n",
    "        \n",
    "#         nodes_left = torch.numel(mask) - int(torch.sum(mask, dim=[0, 1, 2]).item())\n",
    "        \n",
    "#         # Keep from deleting all nodes in a layer\n",
    "#         if nodes_left > min_nodes:\n",
    "#             layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "#             num_params_pruned += (1024 * num_heads * 3)\n",
    "#             num_params_pruned += (1024 * num_heads * 3)\n",
    "#             node_num += (num_heads * 3)\n",
    "#         else:\n",
    "#             stop_pruning_layer = True "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tap",
   "language": "python",
   "name": "tap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
