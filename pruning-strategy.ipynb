{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec25fe-bed3-43a9-94e8-22413302a7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from node_attribution.gradient_node_attribution import get_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289d2a47-0d93-43ad-897b-3ae05dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"560m\"\n",
    "data = [\"Oh, interesting, I am not familiar with that movie! Can you tell me more about it?\"]\n",
    "prune_percent = 0.1\n",
    "num_params_to_prune = 559214592 * prune_percent\n",
    "\n",
    "head_dim = 64\n",
    "num_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efbe0a3-048c-4ca6-a77e-dfe9315161ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading bigscience/bloom-560m.\n",
      "                    Num Transformer Blocks: 24\n",
      "                    Num Attention Heads: 16\n",
      "                    Head Dim: 64\n",
      "                    Hidden Size: 1024\n",
      "                    Base Model Param Count: 559,214,592\n",
      "                    Total Param Count (w/ LM Head): 816,115,712\n"
     ]
    }
   ],
   "source": [
    "# Get attributions\n",
    "avg_contributions, max_contributions, final_contributions, model, model_params = get_attributions(model_size, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b7a08a-65d7-4158-9e60-fd6839f59c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take absolute value of average contributions\n",
    "for layer_name, contribution_tensor in final_contributions.items():\n",
    "    final_contributions[layer_name] = torch.abs(final_contributions[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e880a64-b350-4423-9298-5ef32f86976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "params_to_index = {}\n",
    "for param_name in model_params.keys():\n",
    "    if \"bias\" not in param_name:\n",
    "        params_to_index[param_name] = index\n",
    "        index += 1\n",
    "    \n",
    "index_to_params = {params_to_index[param_name]: param_name for param_name in params_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830cf301-0660-43f4-839c-02016f3bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shape_map = {}\n",
    "layer_names, contribution_tensors = zip(*final_contributions.items())\n",
    "num_layers = len(layer_names)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer_name = layer_names[i]\n",
    "    layer_size = contribution_tensors[i].shape[0]\n",
    "    \n",
    "    if i != 0:\n",
    "        next_layer_size = contribution_tensors[i - 1].shape[0]\n",
    "    else:\n",
    "        next_layer_size = 250880\n",
    "        \n",
    "    \n",
    "    if i != (num_layers - 1):\n",
    "        prev_layer_size = contribution_tensors[i + 1].shape[0]\n",
    "    else:\n",
    "        prev_layer_size = 1024\n",
    "        \n",
    "    layer_shape_map[layer_name] = {\n",
    "        \"prev_layer_size\": prev_layer_size,\n",
    "        \"next_layer_size\": next_layer_size,\n",
    "        \"current_layer_size\": layer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52304b74-b166-4c13-a95f-2eafc0c3a705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "for layer_name, contribution_tensor in final_contributions.items():\n",
    "    for node_id, node in enumerate(contribution_tensor.tolist()):\n",
    "        node_name = f\"{layer_name}.{node_id}\"\n",
    "        all_nodes.append((node_name, node))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fee0d3a6-0b98-4e18-9940-34b0f3ed7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all the nodes\n",
    "all_nodes.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7061e785-c991-4e60-b4c3-80bfb778cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = []\n",
    "\n",
    "# Figure out which layers have the most to prune\n",
    "for layer_name, contribution_tensor in final_contributions.items():\n",
    "    \n",
    "    # Don't prune self_attention.dense.weight directly, use value matrix to decide what to prune\n",
    "    if \"self_attention.dense.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"mlp.dense_h_to_4h.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"self_attention.query_key_value.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"transformer.ln_f.weight\" in layer_name:\n",
    "        continue\n",
    "        \n",
    "    if \"lm_head.weight\" in layer_name:\n",
    "        continue\n",
    "    \n",
    "    # Get average contribution over the whole layer\n",
    "    mean_contribution = torch.mean(contribution_tensor, 0).item()\n",
    "    all_layers.append((layer_name, mean_contribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6568baf2-3382-4e41-a309-e20b827d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6f75b8e-7803-464c-b368-21c453ba0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_masks = {}\n",
    "num_params_pruned = 0\n",
    "node_num = 0\n",
    "min_nodes = 24\n",
    "\n",
    "while num_params_pruned < num_params_to_prune:\n",
    "    lowest_contr_layer_name = all_layers[0][0]               \n",
    "    shapes = layer_shape_map[lowest_contr_layer_name] \n",
    "    stop_pruning_layer = False\n",
    "    \n",
    "    # If the layer is a query_key_value_fused_output layer\n",
    "    if \"query_key_value_fused_output\" in lowest_contr_layer_name:\n",
    "\n",
    "        # Build pruning mask\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = final_contributions[lowest_contr_layer_name]\n",
    "            layer_contributions = layer_contributions.view(num_heads, 3, head_dim)\n",
    "\n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            st = torch.sort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "        # Prune 3 * num head nodes at once, (this will decrease head dim by 1 for each atten head in this layer)\n",
    "        # Make sure each of the 3 nodes are in the 3 different query, key, and value regions\n",
    "\n",
    "        # TODO: Find a more efficient way of doing this\n",
    "        for head in range(num_heads):\n",
    "            for qkv in range(3):\n",
    "                index_to_mask = sorted_contributions[head][qkv][num_pruned]\n",
    "                mask[head][qkv][index_to_mask] = 1\n",
    "        \n",
    "        nodes_left = torch.numel(mask) - int(torch.sum(mask, dim=[0, 1, 2]).item())\n",
    "        \n",
    "        # Keep from deleting all nodes in a layer\n",
    "        if nodes_left > min_nodes:\n",
    "            layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "            num_params_pruned += (shapes[\"prev_layer_size\"] * num_heads * 3)\n",
    "            num_params_pruned += (shapes[\"next_layer_size\"] * num_heads * 3)\n",
    "            node_num += (num_heads * 3)\n",
    "        else:\n",
    "            stop_pruning_layer = True\n",
    "\n",
    "    else:\n",
    "        # Prune one node at time\n",
    "        if lowest_contr_layer_name not in layer_masks:\n",
    "            layer_contributions = final_contributions[lowest_contr_layer_name]\n",
    "            mask = torch.zeros_like(layer_contributions)\n",
    "            sorted_contributions = torch.argsort(layer_contributions)\n",
    "            num_pruned = 0\n",
    "\n",
    "        else:\n",
    "            mask = layer_masks[lowest_contr_layer_name][0]\n",
    "            sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "            num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "            \n",
    "        index_to_mask = sorted_contributions[num_pruned]\n",
    "        mask[index_to_mask] = 1\n",
    "        \n",
    "        nodes_left = torch.numel(mask) - int(torch.sum(mask).item())\n",
    "        \n",
    "        # Keep from deleting all nodes in a layer\n",
    "        if nodes_left > min_nodes:\n",
    "            layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "            # num_params_pruned += shapes[\"prev_layer_size\"]\n",
    "            # num_params_pruned += shapes[\"next_layer_size\"]\n",
    "            num_params_pruned += 2048\n",
    "            node_num += 1\n",
    "        else:\n",
    "            stop_pruning_layer = True\n",
    "\n",
    "    # Apply mask and update the layer mean in \"all_layers\"\n",
    "    if stop_pruning_layer:\n",
    "        new_layer_contr_score = float('inf')\n",
    "    else:\n",
    "        mean_array = np.ma.array(data=final_contributions[lowest_contr_layer_name], mask=mask)\n",
    "        new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    # print(all_layers[0])\n",
    "    all_layers[0] = (lowest_contr_layer_name, new_layer_contr_score)\n",
    "    # print(all_layers[0])\n",
    "    # print(f\"Num params removed: {num_params_pruned}\")\n",
    "    # print(f\"Num Nodes removed: {node_num}\")\n",
    "    # print(\"=====\")\n",
    "    \n",
    "    # re-sort layers now that this one has been pruned and pick the lowest contributing layer again\n",
    "    all_layers.sort(key = lambda x:x[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50ec7830-fe51-4256-bbe6-2412d244a3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.h.0.mlp.dense_4h_to_h.weight': (tensor([1., 1., 0.,  ..., 1., 1., 1.]),\n",
       "  tensor([ 765, 1848, 2195,  ..., 2008,  886, 2743]),\n",
       "  3701),\n",
       " 'transformer.h.1.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       "  tensor([2047, 3960, 3580,  ..., 1081, 2134,  440]),\n",
       "  3291),\n",
       " 'transformer.h.9.mlp.dense_4h_to_h.weight': (tensor([1., 0., 1.,  ..., 1., 0., 1.]),\n",
       "  tensor([2785, 4004, 1597,  ..., 4006,  815, 1594]),\n",
       "  2499),\n",
       " 'transformer.h.2.mlp.dense_4h_to_h.weight': (tensor([1., 1., 0.,  ..., 1., 1., 1.]),\n",
       "  tensor([3937, 1727,  419,  ..., 3316, 3389,  312]),\n",
       "  3197),\n",
       " 'transformer.h.3.mlp.dense_4h_to_h.weight': (tensor([1., 1., 1.,  ..., 0., 1., 1.]),\n",
       "  tensor([2381, 2742,  372,  ..., 2255, 1142, 2044]),\n",
       "  2789),\n",
       " 'transformer.h.8.mlp.dense_4h_to_h.weight': (tensor([0., 1., 1.,  ..., 0., 0., 1.]),\n",
       "  tensor([1511, 2702, 3640,  ..., 3622,  665, 1614]),\n",
       "  2080),\n",
       " 'transformer.h.5.mlp.dense_4h_to_h.weight': (tensor([1., 1., 0.,  ..., 1., 1., 1.]),\n",
       "  tensor([ 443, 4043, 2629,  ..., 2979, 3677, 2080]),\n",
       "  2415),\n",
       " 'transformer.h.4.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 1., 1., 0.]),\n",
       "  tensor([1549, 3629,    9,  ..., 1694, 1626, 1825]),\n",
       "  2364),\n",
       " 'transformer.h.7.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 1., 0., 0.]),\n",
       "  tensor([2169,  800, 2567,  ..., 3462,   58, 1792]),\n",
       "  1811),\n",
       " 'transformer.h.6.mlp.dense_4h_to_h.weight': (tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([ 957, 4045,  607,  ..., 3839, 2388, 1374]),\n",
       "  1743),\n",
       " 'transformer.h.10.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 1., 1., 0.]),\n",
       "  tensor([1830, 3780,  194,  ...,  672, 1521, 3234]),\n",
       "  757),\n",
       " 'transformer.h.11.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([1957, 3595, 1073,  ..., 3932, 3478,   41]),\n",
       "  367),\n",
       " 'transformer.h.22.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([3845, 2283, 3151,  ...,  434,  900,  755]),\n",
       "  274),\n",
       " 'transformer.h.23.mlp.dense_4h_to_h.weight': (tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([2108, 2489, 2168,  ...,  127,  353, 3253]),\n",
       "  18)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc0d48da-8f45-4f78-a502-ed4e413b6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Line up weights to prune and weights in the state dict\n",
    "mask_index = 0\n",
    "sorted_weight_index = 1\n",
    "pruned_model_params = model_params.copy()\n",
    "\n",
    "for layer_name in layer_masks.keys():\n",
    "    if layer_name == \"transformer.h.0.self_attention.query_key_value.weight\":\n",
    "        continue\n",
    "    elif \"query_key_value_fused_output\" in layer_name:   \n",
    "        # Prune as input\n",
    "        # Look at value matrix to decide what should be droped in \"self_attention.dense.weight\"\n",
    "        value_reshape_mask = layer_masks[layer_name][mask_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        num_nodes_to_drop = int(sum(value_reshape_mask).item())\n",
    "        value_indices = layer_masks[layer_name][sorted_weight_index].transpose(0, 1)[-1].reshape(num_heads * head_dim)\n",
    "        value_keep_index = torch.sort(value_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        dense_layer_name = layer_name.replace(\"query_key_value_fused_output\", \"dense\")\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[dense_layer_name], -1, value_keep_index)\n",
    "        pruned_model_params[dense_layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Re-arrange mask to flatten shape\n",
    "        reshaped_mask = layer_masks[layer_name][mask_index].view(num_heads * 3 * head_dim)\n",
    "        rehsaped_indices = layer_masks[layer_name][sorted_weight_index].view(num_heads * 3 * head_dim)\n",
    "        num_nodes_to_drop = int(sum(reshaped_mask).item())\n",
    "        keep_index = torch.sort(rehsaped_indices[num_nodes_to_drop:]).values\n",
    "        \n",
    "        # Prune as output\n",
    "        prev_layer_name = layer_name.replace(\"query_key_value_fused_output\", \"query_key_value\")\n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "        \n",
    "    else:\n",
    "        # Prune when nodes are the input\n",
    "        num_nodes_to_drop = int(sum(layer_masks[layer_name][mask_index]).item())\n",
    "        keep_index = torch.sort(layer_masks[layer_name][sorted_weight_index][num_nodes_to_drop:]).values\n",
    "        pruned_input_weights = torch.index_select(pruned_model_params[layer_name], -1, keep_index)\n",
    "        pruned_model_params[layer_name] = pruned_input_weights\n",
    "        \n",
    "        # Go to previous layer and prune when nodes are the output\n",
    "        prev_layer_index = params_to_index[layer_name] - 1\n",
    "        prev_layer_name = index_to_params[prev_layer_index]\n",
    "        \n",
    "        if \"layernorm\" in prev_layer_name:\n",
    "            pruned_layer_norm_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "            pruned_model_params[prev_layer_name] = pruned_layer_norm_weights \n",
    "            \n",
    "            # Also do bias term\n",
    "            bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "            pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "            pruned_model_params[bias_layer_name] = pruned_bias_weights\n",
    "            \n",
    "            prev_layer_index = prev_layer_index - 1\n",
    "            prev_layer_name = index_to_params[prev_layer_index]\n",
    "            \n",
    "        pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "        pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "        \n",
    "        # Also do bias term\n",
    "        bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "        pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "        pruned_model_params[bias_layer_name] = pruned_bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb4a783-6cd1-44f9-b4e9-a3dd2f5b0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model_params, \"pruned_560m_bloom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b70a7e7-b902-4ecd-8c82-bb0764f3b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ae74350-2038-435f-b708-e38f0cdee675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight torch.Size([250880, 1024])\n",
      "transformer.word_embeddings_layernorm.weight torch.Size([1024])\n",
      "transformer.word_embeddings_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.0.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.0.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.Size([395, 1024])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias torch.Size([395])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.Size([1024, 395])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.1.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.1.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.1.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.Size([805, 1024])\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias torch.Size([805])\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.Size([1024, 805])\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.2.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.2.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.2.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.Size([899, 1024])\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias torch.Size([899])\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.Size([1024, 899])\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.3.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.3.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.3.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.Size([1307, 1024])\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias torch.Size([1307])\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.Size([1024, 1307])\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.4.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.4.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.4.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.Size([1732, 1024])\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias torch.Size([1732])\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.Size([1024, 1732])\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.5.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.5.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.5.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.Size([1681, 1024])\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias torch.Size([1681])\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.Size([1024, 1681])\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.6.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.6.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.Size([2353, 1024])\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias torch.Size([2353])\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.Size([1024, 2353])\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.7.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.7.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.Size([2285, 1024])\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias torch.Size([2285])\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.Size([1024, 2285])\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.8.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.8.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.Size([2016, 1024])\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias torch.Size([2016])\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.Size([1024, 2016])\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.9.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.9.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.9.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.Size([1597, 1024])\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias torch.Size([1597])\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.Size([1024, 1597])\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.10.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.10.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.Size([3339, 1024])\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias torch.Size([3339])\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.Size([1024, 3339])\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.11.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.11.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.11.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.Size([3729, 1024])\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias torch.Size([3729])\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.Size([1024, 3729])\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.12.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.12.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.12.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.13.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.13.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.14.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.14.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.15.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.15.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.15.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.16.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.16.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.16.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.17.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.17.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.17.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.18.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.18.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.18.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.19.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.19.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.19.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.20.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.20.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.20.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.21.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.21.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.21.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.Size([4096, 1024])\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias torch.Size([4096])\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.Size([1024, 4096])\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.22.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.22.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.22.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.Size([3822, 1024])\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias torch.Size([3822])\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.Size([1024, 3822])\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.input_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.Size([3072, 1024])\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.Size([3072])\n",
      "transformer.h.23.self_attention.dense.weight torch.Size([1024, 1024])\n",
      "transformer.h.23.self_attention.dense.bias torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.weight torch.Size([1024])\n",
      "transformer.h.23.post_attention_layernorm.bias torch.Size([1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.Size([4078, 1024])\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias torch.Size([4078])\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.Size([1024, 4078])\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.weight torch.Size([250880, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pruned_model_params.keys():\n",
    "    state_dict_shapes[param_name] = pruned_model_params[param_name].shape\n",
    "    print(param_name, pruned_model_params[param_name].shape)\n",
    "    \n",
    "pkl.dump(state_dict_shapes, open(\"state_dict_shapes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28af6ba2-5c54-4693-a1ba-56cc2d8da64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27777099609375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num / len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81c1d532-4970-4891-88a0-b1110c867364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27306"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeed23-709d-4bc9-b439-e977deb53796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tap",
   "language": "python",
   "name": "tap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
