{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec25fe-bed3-43a9-94e8-22413302a7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from node_attribution.blender_bot_gradient_node_attribution import get_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5f02b6-a727-47ed-baf0-14ae880caa13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_filtered_pairs = pkl.load(open(\"44_human_filtered_conv_pairs.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cc4ca1-1604-4734-925b-39eed900da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data = human_filtered_pairs[:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efbe0a3-048c-4ca6-a77e-dfe9315161ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading facebook/blenderbot-400M-distill.\n",
      "                    Num Encoder Transformer Blocks: 2\n",
      "                    Num Decoder Transformer Blocks: 12\n",
      "                    Num Encoder Attention Heads: 32\n",
      "                    Num Decoder Attention Heads: 32\n",
      "                    Encoder Head Dim: 40\n",
      "                    Decoder Head Dim: 40\n",
      "                    Hidden Size: 1280\n",
      "                    Base Model Param Count: 364,802,560\n",
      "                    Total Param Count (w/ LM Head): 375,052,800\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# Get attributions\n",
    "model_size = \"400M-distill\"\n",
    "avg_contributions, max_contributions, model, model_params = get_attributions(model_size, calibration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b7a08a-65d7-4158-9e60-fd6839f59c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take absolute value of average contributions\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    avg_contributions[layer_name] = torch.abs(avg_contributions[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7840688f-5c3f-4670-b280-0e7a26cef4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(avg_contributions, open(\"avg_contri_blender_bot_400M_22pair_calibration.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57fcb4a-9f44-48d3-9267-5620a8dcfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(max_contributions, open(\"max_contri_blender_bot_400M_22pair_calibration.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "289d2a47-0d93-43ad-897b-3ae05dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_percent = 0.0\n",
    "num_params_to_prune = 375052800 * prune_percent\n",
    "\n",
    "head_dim = 40\n",
    "num_heads = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e880a64-b350-4423-9298-5ef32f86976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "params_to_index = {}\n",
    "for param_name in model_params.keys():\n",
    "    if \"bias\" not in param_name:\n",
    "        params_to_index[param_name] = index\n",
    "        index += 1\n",
    "    \n",
    "index_to_params = {params_to_index[param_name]: param_name for param_name in params_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f993bcaa-4ce4-45bf-b076-63710af5d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    if \"fc2.weight\" in layer_name:\n",
    "        for node_id, node in enumerate(contribution_tensor.tolist()):\n",
    "            node_name = f\"{layer_name}.{node_id}\"\n",
    "            all_nodes.append((node_name, node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7061e785-c991-4e60-b4c3-80bfb778cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = []\n",
    "\n",
    "# Figure out which layers have the most to prune\n",
    "for layer_name, contribution_tensor in avg_contributions.items():\n",
    "    \n",
    "    if \"fc2.weight\" in layer_name:\n",
    "        # Get average contribution over the whole layer\n",
    "        mean_contribution = torch.mean(contribution_tensor, 0).item()\n",
    "        all_layers.append((layer_name, mean_contribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6568baf2-3382-4e41-a309-e20b827d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4c11859-bded-4210-8fbd-2a5cae586151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.encoder.layers.0.fc2.weight', 0.00012805909500457346),\n",
       " ('model.encoder.layers.1.fc2.weight', 0.00016461317136418074),\n",
       " ('model.decoder.layers.0.fc2.weight', 0.0002610544615890831),\n",
       " ('model.decoder.layers.1.fc2.weight', 0.00031016708817332983),\n",
       " ('model.decoder.layers.2.fc2.weight', 0.00037199354846961796),\n",
       " ('model.decoder.layers.3.fc2.weight', 0.00040806649485602975),\n",
       " ('model.decoder.layers.4.fc2.weight', 0.00044847078970633447),\n",
       " ('model.decoder.layers.9.fc2.weight', 0.00045117439003661275),\n",
       " ('model.decoder.layers.8.fc2.weight', 0.0004720468132290989),\n",
       " ('model.decoder.layers.10.fc2.weight', 0.00047606881707906723),\n",
       " ('model.decoder.layers.7.fc2.weight', 0.0004931202274747193),\n",
       " ('model.decoder.layers.5.fc2.weight', 0.000497086439281702),\n",
       " ('model.decoder.layers.6.fc2.weight', 0.0005099925328977406),\n",
       " ('model.decoder.layers.11.fc2.weight', 0.0005775314057245851)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f75b8e-7803-464c-b368-21c453ba0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_masks = {}\n",
    "num_params_pruned = 0\n",
    "node_num = 0\n",
    "min_nodes = 24\n",
    "\n",
    "while num_params_pruned < num_params_to_prune:\n",
    "    lowest_contr_layer_name = all_layers[0][0]               \n",
    "    stop_pruning_layer = False\n",
    " \n",
    "    # Prune one node at time\n",
    "    if lowest_contr_layer_name not in layer_masks:\n",
    "        layer_contributions = avg_contributions[lowest_contr_layer_name]\n",
    "        mask = torch.zeros_like(layer_contributions)\n",
    "        sorted_contributions = torch.argsort(layer_contributions)\n",
    "        num_pruned = 0\n",
    "\n",
    "    else:\n",
    "        mask = layer_masks[lowest_contr_layer_name][0]\n",
    "        sorted_contributions = layer_masks[lowest_contr_layer_name][1]\n",
    "        num_pruned = layer_masks[lowest_contr_layer_name][2]\n",
    "\n",
    "    index_to_mask = sorted_contributions[num_pruned]\n",
    "    mask[index_to_mask] = 1\n",
    "\n",
    "    nodes_left = torch.numel(mask) - int(torch.sum(mask).item())\n",
    "\n",
    "    # Keep from deleting all nodes in a layer\n",
    "    if nodes_left > min_nodes:\n",
    "        layer_masks[lowest_contr_layer_name] = (mask, sorted_contributions, num_pruned + 1)\n",
    "        num_params_pruned += 2560\n",
    "        node_num += 1\n",
    "    else:\n",
    "        stop_pruning_layer = True\n",
    "\n",
    "    # Apply mask and update the layer mean in \"all_layers\"\n",
    "    if stop_pruning_layer:\n",
    "        new_layer_contr_score = float('inf')\n",
    "    else:\n",
    "        mean_array = np.ma.array(data=avg_contributions[lowest_contr_layer_name], mask=mask)\n",
    "        new_layer_contr_score = mean_array.mean()\n",
    "\n",
    "    all_layers[0] = (lowest_contr_layer_name, new_layer_contr_score)\n",
    "\n",
    "    # re-sort layers now that this one has been pruned and pick the lowest contributing layer again\n",
    "    all_layers.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0206985a-0ff8-4ce5-851d-51db90587410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc0d48da-8f45-4f78-a502-ed4e413b6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Line up weights to prune and weights in the state dict\n",
    "mask_index = 0\n",
    "sorted_weight_index = 1\n",
    "pruned_model_params = model_params.copy()\n",
    "\n",
    "for layer_name in layer_masks.keys():\n",
    "    # Prune when nodes are the input\n",
    "    num_nodes_to_drop = int(sum(layer_masks[layer_name][mask_index]).item())\n",
    "    keep_index = torch.sort(layer_masks[layer_name][sorted_weight_index][num_nodes_to_drop:]).values\n",
    "    pruned_input_weights = torch.index_select(pruned_model_params[layer_name], -1, keep_index)\n",
    "    pruned_model_params[layer_name] = pruned_input_weights\n",
    "\n",
    "    # Go to previous layer and prune when nodes are the output\n",
    "    prev_layer_index = params_to_index[layer_name] - 1\n",
    "    prev_layer_name = index_to_params[prev_layer_index]\n",
    "\n",
    "    pruned_output_weights = torch.index_select(pruned_model_params[prev_layer_name], 0, keep_index)\n",
    "    pruned_model_params[prev_layer_name] = pruned_output_weights\n",
    "\n",
    "    # Also do bias term\n",
    "    bias_layer_name = prev_layer_name.replace(\"weight\", \"bias\")\n",
    "    pruned_bias_weights = torch.index_select(pruned_model_params[bias_layer_name], 0, keep_index)\n",
    "    pruned_model_params[bias_layer_name] = pruned_bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdb4a783-6cd1-44f9-b4e9-a3dd2f5b0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model_params, \"pruned_400m_blender_bot2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b70a7e7-b902-4ecd-8c82-bb0764f3b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ae74350-2038-435f-b708-e38f0cdee675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_logits_bias torch.Size([1, 8008])\n",
      "model.shared.weight torch.Size([8008, 1280])\n",
      "model.encoder.embed_tokens.weight torch.Size([8008, 1280])\n",
      "model.encoder.embed_positions.weight torch.Size([128, 1280])\n",
      "model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.0.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.0.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.0.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.0.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.encoder.layers.0.fc1.weight torch.Size([5120, 1280])\n",
      "model.encoder.layers.0.fc1.bias torch.Size([5120])\n",
      "model.encoder.layers.0.fc2.weight torch.Size([1280, 5120])\n",
      "model.encoder.layers.0.fc2.bias torch.Size([1280])\n",
      "model.encoder.layers.0.final_layer_norm.weight torch.Size([1280])\n",
      "model.encoder.layers.0.final_layer_norm.bias torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.1.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.1.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.1.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.encoder.layers.1.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.encoder.layers.1.fc1.weight torch.Size([5120, 1280])\n",
      "model.encoder.layers.1.fc1.bias torch.Size([5120])\n",
      "model.encoder.layers.1.fc2.weight torch.Size([1280, 5120])\n",
      "model.encoder.layers.1.fc2.bias torch.Size([1280])\n",
      "model.encoder.layers.1.final_layer_norm.weight torch.Size([1280])\n",
      "model.encoder.layers.1.final_layer_norm.bias torch.Size([1280])\n",
      "model.encoder.layer_norm.weight torch.Size([1280])\n",
      "model.encoder.layer_norm.bias torch.Size([1280])\n",
      "model.decoder.embed_tokens.weight torch.Size([8008, 1280])\n",
      "model.decoder.embed_positions.weight torch.Size([128, 1280])\n",
      "model.decoder.layers.0.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.0.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.0.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.0.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.0.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.0.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.0.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.1.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.1.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.1.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.1.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.1.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.1.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.2.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.2.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.2.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.2.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.2.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.2.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.3.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.3.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.3.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.3.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.3.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.3.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.4.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.4.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.4.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.4.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.4.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.4.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.5.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.5.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.5.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.5.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.5.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.5.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.6.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.6.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.6.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.6.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.6.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.6.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.7.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.7.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.7.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.7.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.7.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.7.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.8.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.8.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.8.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.8.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.8.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.8.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.9.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.9.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.9.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.9.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.9.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.9.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.10.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.10.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.10.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.10.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.10.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.10.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.self_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.self_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.self_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.self_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.encoder_attn.k_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight torch.Size([1280, 1280])\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layers.11.fc1.weight torch.Size([5120, 1280])\n",
      "model.decoder.layers.11.fc1.bias torch.Size([5120])\n",
      "model.decoder.layers.11.fc2.weight torch.Size([1280, 5120])\n",
      "model.decoder.layers.11.fc2.bias torch.Size([1280])\n",
      "model.decoder.layers.11.final_layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layers.11.final_layer_norm.bias torch.Size([1280])\n",
      "model.decoder.layer_norm.weight torch.Size([1280])\n",
      "model.decoder.layer_norm.bias torch.Size([1280])\n",
      "lm_head.weight torch.Size([8008, 1280])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pruned_model_params.keys():\n",
    "    state_dict_shapes[param_name] = pruned_model_params[param_name].shape\n",
    "    print(param_name, pruned_model_params[param_name].shape)\n",
    "    \n",
    "pkl.dump(state_dict_shapes, open(\"pruned_400m_blender_bot2_state_dict_shapes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28af6ba2-5c54-4693-a1ba-56cc2d8da64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num / len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c1d532-4970-4891-88a0-b1110c867364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207a091-f70d-47c7-9b78-1bc42a328e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff6b26-2dd6-4776-8cd7-d7551442b756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tap",
   "language": "python",
   "name": "tap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
